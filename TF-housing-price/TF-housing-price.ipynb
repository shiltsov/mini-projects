{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing price\n",
    "\n",
    "https://storage.googleapis.com/ml_universities/california_housing_train.csv\n",
    "\n",
    "что хочу:\n",
    "    - импортировать в пандас, посмотреть что как, очистить, разбить на трейн и тест\n",
    "    - построить TF датасет, поработать с фичами\n",
    "    - построить и обучить денс-нн, учить с коллбеками, с сохранением слоев и с валидацией\n",
    "    - на коллбеках сохраняться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем на трейн и валидацию\n",
    "df_train, df_test = train_test_split(df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_median_age_mean = df_train['housing_median_age'].mean()\n",
    "housing_median_age_std =  df_train['housing_median_age'].std()\n",
    "\n",
    "median_income_mean = df_train['median_income'].mean()\n",
    "median_income_std = df_train['median_income'].std()\n",
    "\n",
    "# эти данные буду использовать для нормализации\n",
    "housing_median_age_mean, housing_median_age_std, median_income_mean, median_income_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# то, что предсказываем\n",
    "target_train = df_train.pop('median_house_value')\n",
    "target_test = df_test.pop('median_house_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с фичами\n",
    "    \n",
    "    - хочу на лету создавать новые фичи - среднее число комнат на дом, среднее число жильцов на комнату\n",
    "    - longitude и latitude сначала делаем бакетами, потом делаем из них крест. Т.к у нас признаком являются не сами числа, а принадлежность к квадратику\n",
    "    - нормализовать все данными из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['longitude', 'latitude', 'housing_median_age', 'population'\n",
    "            'total_rooms', 'total_bedrooms','households','median_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прекрасно, прекрасно справляется :)\n",
    "def make_new_features(x, y):\n",
    "    x['family_size'] = x['population'] / x['households']\n",
    "    x['pop_per_room'] = x['population'] / x['total_rooms']\n",
    "    x['pop_per_bedroom'] = x['population'] / x['total_bedrooms']\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нам это нужно и для трейна и для предикта\n",
    "def create_dataset(d, t):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((d.to_dict('list'), t.values)).batch(4)\n",
    "    ds_featured = ds.map(make_new_features)\n",
    "    return ds_featured\n",
    "    #layer = tf.keras.layers.DenseFeatures(features_list)\n",
    "    \n",
    "ds = create_dataset(df_train, target_train)\n",
    "ds_train = create_dataset(df_train, target_train)\n",
    "ds_test = create_dataset(df_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds.take(1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_column имеют немного другую роль. Они потом превращаются в слой модели,\n",
    "# т.е по сути на старте они ничего и не знают про датасет, модель будет пихать в них\n",
    "# данные как в мясорубку\n",
    "\n",
    "num_features_unnorm = ['family_size', 'pop_per_room', 'pop_per_bedroom',\n",
    "                       'total_rooms', 'total_bedrooms', 'households']\n",
    "\n",
    "def create_features():\n",
    "    # не важно что мы тут делаем, важно что выводим в виде итогового массива\n",
    "    latitude = tf.feature_column.bucketized_column(\n",
    "            tf.feature_column.numeric_column('latitude'), boundaries = np.arange(32.0, 42, 1).tolist())    \n",
    "    \n",
    "    longitude = tf.feature_column.bucketized_column(\n",
    "            tf.feature_column.numeric_column('longitude'), boundaries = np.arange(-122, -114, 1).tolist())  \n",
    "    \n",
    "    squares = tf.feature_column.indicator_column(\n",
    "        tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=1000))\n",
    "\n",
    "    # нормализатор там встроен, пробую\n",
    "    def norm_housing_median_age(x):\n",
    "        return (x - housing_median_age_mean)/ housing_median_age_std\n",
    "    housing_median_age = tf.feature_column.numeric_column('housing_median_age', \n",
    "                                                          normalizer_fn = norm_housing_median_age)\n",
    "\n",
    "    def norm_median_income(x):\n",
    "        return (x - median_income_mean)/ median_income_std\n",
    "    median_income = tf.feature_column.numeric_column('median_income', \n",
    "                                                          normalizer_fn = norm_median_income)\n",
    "\n",
    "    num_features = [tf.feature_column.numeric_column(name) for name in num_features_unnorm]\n",
    "    \n",
    "    return [housing_median_age, median_income, latitude] + num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseFeatures по сути просто делает тензор, на входе ему нужно подать массив \"ленточек\" из которых делать\n",
    "# а ленточки определяются через feature_columns\n",
    "\n",
    "layer = tf.keras.layers.DenseFeatures(create_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds.take(1):\n",
    "    print(layer(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель, обучение, коллбеки и вот это вот все"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layer,\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение\n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint('./save/model.ckpt', save_weights_only=True, verbose=1)\n",
    "\n",
    "# тензорбоард\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./save/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train, epochs=10, validation_data=ds_test, callbacks=[save_callback, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir='./save'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
